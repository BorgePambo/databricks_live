# Pipeline de Segmenta√ß√£o de Clientes - Lakehouse Flow

## üìã Vis√£o Geral

Este pipeline implementa uma solu√ß√£o completa de ETL usando **Lakehouse Flow (Delta Live Tables)** do Databricks para segmenta√ß√£o de clientes baseada em comportamento transacional.

## üèóÔ∏è Arquitetura

```
üìÅ Volumes CSV ‚Üí ü•â Bronze ‚Üí ü•à Silver ‚Üí ü•á Gold
```

### Camadas do Pipeline

1. **Bronze**: Ingest√£o incremental de dados brutos dos Volumes
2. **Silver**: Transforma√ß√µes, normaliza√ß√£o e data quality
3. **Gold**: Agrega√ß√µes de m√©tricas de neg√≥cio e segmenta√ß√£o

## üìÅ Estrutura de Arquivos

```
Pipeline_live/
‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îú‚îÄ‚îÄ customers.sql                  # Ingest√£o de clientes
‚îÇ   ‚îú‚îÄ‚îÄ transacation_btc.sql           # Ingest√£o de transa√ß√µes Bitcoin
‚îÇ   ‚îú‚îÄ‚îÄ transaction_commodities.sql    # Ingest√£o de transa√ß√µes Commodities
‚îÇ   ‚îú‚îÄ‚îÄ quotation_btc.sql              # Ingest√£o de cota√ß√µes Bitcoin
‚îÇ   ‚îî‚îÄ‚îÄ quotation_yfinance.sql        # Ingest√£o de cota√ß√µes yFinance
‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îú‚îÄ‚îÄ fact_transaction_assets.sql    # Fato unificado de transa√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ fact_quotation_assets.sql      # Fato unificado de cota√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ dim_clientes.sql               # Dimens√£o de clientes (anonimizada)
‚îÇ   ‚îî‚îÄ‚îÄ fact_transaction_revenue.sql  # Fato de receita por transa√ß√£o
‚îî‚îÄ‚îÄ gold/
    ‚îî‚îÄ‚îÄ mostvaluableclient.sql         # Segmenta√ß√£o de clientes mais valiosos
```

## ü•â Camada Bronze

### Objetivo
Ingest√£o incremental de dados brutos preservando a estrutura original dos CSVs.

### Caracter√≠sticas
- **Tipo**: `CREATE OR REFRESH STREAMING TABLE`
- **Fonte**: Volumes do Databricks (`/Volumes/lakehouse_live/raw_public/[arquivo]`)
- **Formato**: CSV com header e schema inferido automaticamente
- **Processamento**: Incremental via `cloud_files()`
- **Timestamp**: Campo `ingested_at` registra quando o dado foi processado

### Tabelas Bronze

| Tabela | Volume CSV | Descri√ß√£o |
|--------|------------|-----------|
| `bronze.customers` | `/Volumes/lakehouse_live/raw_public/customers` | Dados de clientes |
| `bronze.transaction_btc` | `/Volumes/lakehouse_live/raw_public/transacation_btc` | Transa√ß√µes Bitcoin |
| `bronze.transaction_commodities` | `/Volumes/lakehouse_live/raw_public/transaction_commodities` | Transa√ß√µes Commodities |
| `bronze.quotation_btc` | `/Volumes/lakehouse_live/raw_public/quotation_btc` | Cota√ß√µes Bitcoin |
| `bronze.quotation_yfinance` | `/Volumes/lakehouse_live/raw_public/quotation_yfinance` | Cota√ß√µes yFinance |

## ü•à Camada Silver

### Objetivo
Transforma√ß√£o e normaliza√ß√£o dos dados com valida√ß√µes de qualidade.

### Tabelas Silver

#### 1. `silver.fact_transaction_assets`
- **Fonte**: `bronze.transaction_btc` + `bronze.transaction_commodities`
- **Transforma√ß√µes**:
  - Uni√£o de transa√ß√µes BTC e Commodities
  - Padroniza√ß√£o de s√≠mbolos (BTC, GOLD, OIL, SILVER)
  - Normaliza√ß√£o de timestamps
  - Hora aproximada para join com cota√ß√µes
- **Constraints**: Quantidade > 0, data_hora NOT NULL, tipo_operacao v√°lido, asset_symbol v√°lido

#### 2. `silver.fact_quotation_assets`
- **Fonte**: `bronze.quotation_btc` + `bronze.quotation_yfinance`
- **Transforma√ß√µes**:
  - Uni√£o de cota√ß√µes BTC e yFinance
  - Padroniza√ß√£o de s√≠mbolos
  - Normaliza√ß√£o de timestamps
  - Hora aproximada para join com transa√ß√µes
- **Constraints**: Pre√ßo > 0, hor√°rio v√°lido, ativo NOT NULL, moeda = USD

#### 3. `silver.dim_clientes`
- **Fonte**: `bronze.customers`
- **Transforma√ß√µes**:
  - Anonimiza√ß√£o de documentos (SHA2)
  - Valida√ß√£o de segmentos, pa√≠ses e estados
  - Cria√ß√£o de surrogate key (customer_sk)
- **Constraints**: customer_id NOT NULL, segmento v√°lido, pa√≠s v√°lido

#### 4. `silver.fact_transaction_revenue`
- **Fonte**: `fact_transaction_assets` + `fact_quotation_assets` + `dim_clientes`
- **Transforma√ß√µes**:
  - Join de transa√ß√µes com cota√ß√µes (por hora aproximada e s√≠mbolo)
  - Join com dimens√£o de clientes
  - C√°lculo de valor bruto (quantidade √ó pre√ßo)
  - C√°lculo de receita de taxa (0.25% sobre valor bruto)
  - Aplica√ß√£o de sinal para COMPRA (-) e VENDA (+)
- **Constraints**: gross_value > 0, fee_revenue > 0, customer_sk NOT NULL, cota√ß√£o v√°lida

## ü•á Camada Gold

### Objetivo
Agrega√ß√£o de m√©tricas de neg√≥cio e segmenta√ß√£o de clientes.

### Tabela Gold

#### `gold.mostvaluableclient`
- **Fonte**: `silver.fact_transaction_revenue`
- **M√©tricas Calculadas**:
  - Total de transa√ß√µes por cliente
  - Valor total das transa√ß√µes
  - Ticket m√©dio
  - Primeira e √∫ltima transa√ß√£o
  - Transa√ß√µes nos √∫ltimos 30 dias
  - Receita total de taxas (comiss√µes)
  - Ranking por volume de transa√ß√µes
  - Classifica√ß√£o: Top 1, Top 2, Top 3 ou Outros

## üîÑ Processamento Incremental

Todas as tabelas Silver e Gold utilizam:
- **Tipo**: `CREATE OR REFRESH STREAMING TABLE`
- **Fonte**: `FROM STREAM(tabela_origem)`
- **Benef√≠cio**: Processamento incremental e evita erros de batch query

## üîí Data Quality

### Constraints Implementados

**Sintaxe Oficial:**
```sql
CONSTRAINT nome_valid EXPECT (condicao) ON VIOLATION DROP ROW
```

**A√ß√µes de Viola√ß√£o:**
- `ON VIOLATION DROP ROW`: Remove registros inv√°lidos
- Logs autom√°ticos de viola√ß√µes
- Monitoramento via UI do Databricks

## üõ°Ô∏è Seguran√ßa

- **PII**: Documentos anonimizados com `SHA2(documento, 256)`
- **Governan√ßa**: Unity Catalog
- **Auditoria**: Lakeflow Lineage completo

## üìä Mapeamento de S√≠mbolos

| CSV Original | S√≠mbolo Original | S√≠mbolo Padronizado |
|--------------|------------------|---------------------|
| transaction_btc | BTC | BTC |
| transaction_commodities | GOLD | GOLD |
| transaction_commodities | OIL | OIL |
| transaction_commodities | SILVER | SILVER |
| quotation_btc | BTC-USD | BTC |
| quotation_yfinance | GC=F | GOLD |
| quotation_yfinance | CL=F | OIL |
| quotation_yfinance | SI=F | SILVER |

## üöÄ Como Usar

1. **Configure os Volumes**: Certifique-se de que os arquivos CSV est√£o nos Volumes especificados
2. **Crie o Pipeline**: No Databricks, crie um novo Pipeline usando Lakehouse Flow
3. **Configure o Caminho**: Aponte para a pasta `Pipeline_live` ou suas subpastas
4. **Execute**: Execute o pipeline e monitore via UI do Databricks

## üìö Refer√™ncias

- [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt/)
- [Data Quality Expectations](https://docs.databricks.com/aws/en/dlt/expectations?language=SQL)
- [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/)

---

**Desenvolvido para Jornada de Dados - Pipeline de Segmenta√ß√£o de Clientes**

